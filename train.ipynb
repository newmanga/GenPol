{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train TRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gym_env.environments import create_halfcheetah_env\n",
    "from models.value import ValueNet\n",
    "from models.policy import GaussianPolicyNet\n",
    "from models.trpo import TRPO\n",
    "\n",
    "env = create_halfcheetah_env()\n",
    "env_dim = env.get_dim()\n",
    "\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)[\"HalfCheetah-v5\"]\n",
    "\n",
    "hidden_sizes = (50,50,50)\n",
    "epochs = 2000\n",
    "\n",
    "policy_net = GaussianPolicyNet(env_dim, hidden_sizes)\n",
    "value_net = ValueNet(env_dim[\"states\"], hidden_sizes)\n",
    "trpo = TRPO(env_dim[\"actions\"], policy_net, value_net, config)\n",
    "\n",
    "trpo.train_model(env, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TRPO Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.old_policy import CreatePolicyNet\n",
    "from utils_file import *\n",
    "from gym_env.environments import create_halfcheetah_env\n",
    "\n",
    "temp_env = create_halfcheetah_env()\n",
    "loaded_trpo_policy = CreatePolicyNet(temp_env)\n",
    "loaded_trpo_policy.load_state_dict(torch.load((TRPO_WEIGHTS_PATH / 'policy_net_weights.pth').as_posix()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Expert Data from TRPO policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "seed = 1\n",
    "max_num_steps = 200\n",
    "num_episodes = 50000\n",
    "\n",
    "env = create_halfcheetah_env()\n",
    "\n",
    "trajectories = []\n",
    "\n",
    "with tqdm(total=num_episodes) as pbar:\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "\n",
    "        state = to_tensor(env.reset()[0])\n",
    "        done = False\n",
    "        trunc = False\n",
    "        sum_rewards = 0\n",
    "        trajectory = []\n",
    "\n",
    "        num_steps = 0\n",
    "        while not done and not trunc and num_steps < max_num_steps:\n",
    "\n",
    "            action = policy_net.take_action(state)\n",
    "            next_state, reward, done, trunc = env.take_step(state, action)\n",
    "            # print(t)\n",
    "            next_state = to_tensor(next_state)\n",
    "            sum_rewards += reward\n",
    "\n",
    "            trajectory.append((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            num_steps += 1\n",
    "            # time.sleep(0.1)  # Simulate a delay\n",
    "        states, actions, rewards, next_states = zip(*trajectory)\n",
    "\n",
    "        states = torch.stack(states).squeeze(1)\n",
    "        next_states = torch.stack(next_states).squeeze(1)\n",
    "        actions = torch.stack(actions).squeeze(1)\n",
    "        rewards = torch.as_tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        if (sum_rewards >= 300):\n",
    "            trajectories.append(Rollout(states, actions, rewards, next_states))\n",
    "            \n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.cat([r.states for r in trajectories], dim=0).float()\n",
    "actions = torch.cat([r.actions for r in trajectories], dim=0).float()\n",
    "rewards = torch.cat([r.rewards for r in trajectories], dim=0).float()\n",
    "next_states = torch.cat([r.next_states for r in trajectories], dim=0).float()\n",
    "\n",
    "torch.save(states, EXPERT_DATA_STATES_PATH.as_posix())\n",
    "torch.save(actions, EXPERT_DATA_ACTIONS_PATH.as_posix())\n",
    "torch.save(rewards, EXPERT_DATA_REWARDS_PATH.as_posix())\n",
    "torch.save(next_states, EXPERT_DATA_NEXT_STATES_PATH.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "states = torch.load(EXPERT_DATA_STATES_PATH.as_posix())\n",
    "actions = torch.load(EXPERT_DATA_ACTIONS_PATH.as_posix())\n",
    "rewards = torch.load(EXPERT_DATA_REWARDS_PATH.as_posix())\n",
    "next_states = torch.load(EXPERT_DATA_NEXT_STATES_PATH.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAIL + Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.old_policy import CreatePolicyNet\n",
    "from utils_file import *\n",
    "from gym_env.environments import create_halfcheetah_env\n",
    "\n",
    "temp_env = create_halfcheetah_env()\n",
    "loaded_trpo_policy = CreatePolicyNet(temp_env)\n",
    "loaded_trpo_policy.load_state_dict(torch.load((TRPO_WEIGHTS_PATH / 'policy_net_weights.pth').as_posix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.GAIL import GAIL\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "time = datetime.now().strftime(\"%d_%m_%H_%M_%S\")\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)[\"HalfCheetah-v5\"]\n",
    "\n",
    "env = create_halfcheetah_env(render=False, forward_reward_weight=1)\n",
    "\n",
    "diffusion = True\n",
    "batch_size = 10 # number of rollouts\n",
    "model = GAIL(env, batch_size, diffusion, config).to(device)\n",
    "\n",
    "logs = model.train(loaded_trpo_policy)\n",
    "\n",
    "diffusion_label = \"_diff\" if diffusion else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor\n",
    "\n",
    "real_logprob = torch.exp((-1)*FloatTensor(logs[\"discriminator/real_logprob\"]))\n",
    "fake_logprob = torch.exp((-1)*FloatTensor(logs[\"discriminator/fake_logprob\"]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def moving_average(data, window_size=1):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "def plot_with_std(data, label, window_size=1, color='b'):\n",
    "  # Calculate moving average\n",
    "  average = np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "  # Calculate moving standard deviation\n",
    "  squared_diffs = (data[window_size - 1:] - average)**2\n",
    "  rolling_std = np.sqrt(np.convolve(squared_diffs, np.ones(window_size)/window_size, mode='same'))\n",
    "\n",
    "  # Plot the data with error bars (representing standard deviation)\n",
    "  plt.plot(average, label=label, color=color, linewidth=1)\n",
    "  plt.fill_between(np.arange(len(average)), average - rolling_std, average + rolling_std, alpha=0.2, color=color)\n",
    "\n",
    "# Assuming you have real_fake_logprob and fake_real_logprob arrays\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "\n",
    "plot_with_std(real_logprob.numpy(), label='Exp Data as Fake', window_size=50, color='b')\n",
    "plot_with_std(fake_logprob.numpy(), label='Gen Data as Real', window_size=50, color='g')\n",
    "\n",
    "plt.xlabel('Epochs')  # X-axis label\n",
    "plt.ylabel('Probability')  # Y-axis label\n",
    "plt.title('Discriminator Probability vs Epochs')  # Title of the plot\n",
    "plt.grid(True, linestyle='--', alpha=0.6)  # Add a grid for better readability\n",
    "plt.legend()  # Show legend\n",
    "\n",
    "path = f\"gan_plots/loss_{time}{diffusion_label}.png\"\n",
    "plt.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have real_fake_logprob and fake_real_logprob arrays\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "\n",
    "plot_with_std(logs[\"policy/rewards\"], label='Policy Rewards', window_size=50, color='b')\n",
    "\n",
    "plt.axhline(y=FloatTensor(logs[\"expert/rewards\"]).mean(), color='r', linestyle='--', label='Expert Rewards')\n",
    "\n",
    "plt.xlabel('Epochs')  # X-axis label\n",
    "plt.ylabel('Reward')  # Y-axis label\n",
    "plt.title('Rewards vs Epochs')  # Title of the plot\n",
    "plt.grid(True, linestyle='--', alpha=0.6)  # Add a grid for better readability\n",
    "plt.legend()  # Show legend\n",
    "\n",
    "path = f\"gan_plots/reward_{time}{diffusion_label}.png\"\n",
    "plt.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def try_policy(policy, render=True, max_num_steps=200, time_delay=0.1):\n",
    "    policy.eval()\n",
    "    env = create_halfcheetah_env(render, 1)\n",
    "    ob = env.reset()[0]\n",
    "    steps = 0\n",
    "    ep_rwds = []\n",
    "    done = False\n",
    "    while not done and steps < max_num_steps:\n",
    "        act = policy.take_np_action(ob)\n",
    "        if render:\n",
    "            env.render()\n",
    "        ob, rwd, done, info, _ = env.step(act)\n",
    "\n",
    "        ep_rwds.append(rwd)\n",
    "\n",
    "        steps += 1\n",
    "        time.sleep(time_delay)\n",
    "\n",
    "    print(np.sum(ep_rwds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_policy(model.policy_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BC GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.old_policy import CreatePolicyNet\n",
    "from utils_file import *\n",
    "from gym_env.environments import create_halfcheetah_env\n",
    "\n",
    "temp_env = create_halfcheetah_env()\n",
    "loaded_trpo_policy = CreatePolicyNet(temp_env)\n",
    "loaded_trpo_policy.load_state_dict(torch.load((TRPO_WEIGHTS_PATH / 'policy_net_weights.pth').as_posix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gan import BCGAN\n",
    "import json\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)[\"HalfCheetah-v5\"]\n",
    "\n",
    "env = create_halfcheetah_env(render=False, forward_reward_weight=1)\n",
    "diffusion = True\n",
    "batch_size = 10 # number of rollouts\n",
    "model = BCGAN(env, batch_size, diffusion, config).to(device)\n",
    "\n",
    "model.train(loaded_trpo_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_policy(model.policy_net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
